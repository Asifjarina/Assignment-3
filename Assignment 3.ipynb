{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068f0309",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e1a20",
   "metadata": {},
   "source": [
    "# 1 Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767953f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for: smartphone \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_product_name():\n",
    "  while True:\n",
    "    product_name = input(\"Enter the product you want to search for: \")\n",
    "    if product_name:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Please enter a valid product name.\")\n",
    "  return product_name\n",
    "\n",
    "product_name = get_product_name()\n",
    "\n",
    "base_url = \"https://www.amazon.in/s?k=\"\n",
    "\n",
    "encoded_product_name = product_name.replace(\" \", \"+\")\n",
    "\n",
    "url = base_url + encoded_product_name\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dee24",
   "metadata": {},
   "source": [
    "# 2 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81515370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped product details for 'smartphone ' saved to 'smartphone _products.csv'.\n"
     ]
    }
   ],
   "source": [
    "def get_product_details(item):\n",
    "  brand = item.find(\"span\", class_=\"a-brand\").text.strip() if item.find(\"span\", class_=\"a-brand\") else \"-\"\n",
    "  name = item.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\").text.strip() if item.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\") else \"-\"\n",
    "  price = item.find(\"span\", class_=\"a-offscreen\").text.strip() if item.find(\"span\", class_=\"a-offscreen\") else \"-\"\n",
    "  return_policy = item.find(\"span\", class_=\"a-size-small a-color-secondary\").text.strip() if item.find(\"span\", class_=\"a-size-small a-color-secondary\") else \"-\"\n",
    "  delivery = item.find(\"span\", class_=\"a-size-small a-color-secondary\").find_next(\"span\").text.strip() if item.find(\"span\", class_=\"a-size-small a-color-secondary\").find_next(\"span\") else \"-\"\n",
    "  availability = item.find(\"span\", class_=\"a-size-small a-color-availability\").text.strip() if item.find(\"span\", class_=\"a-size-small a-color-availability\") else \"-\"\n",
    "  url = item.find(\"a\", href=True)[\"href\"]\n",
    "  return brand, name, price, return_policy, delivery, availability, url\n",
    "\n",
    "product_details = []\n",
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "  for item in soup.find_all(\"div\", class_=\"s-result-item s-asin s-sponsored _desktop-」、_móvil_touch s-anime\"):\n",
    "    product_details.append(get_product_details(item))\n",
    "\n",
    " \n",
    "  next_page_button = soup.find(\"a\", title=\"Next\")\n",
    "  if next_page_button:\n",
    "    page_number += 1\n",
    "    url = next_page_button[\"href\"]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "  else:\n",
    "    break\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(product_details, columns=[\"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Product URL\"])\n",
    "df.to_csv(f\"{product_name}_products.csv\", index=False)\n",
    "\n",
    "print(f\"Scraped product details for '{product_name}' saved to '{product_name}_products.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9a00f",
   "metadata": {},
   "source": [
    "# 3 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d29265",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "keywords = [\"fruits\", \"cars\", \"Machine learning\", \"Guitar\", \"Cake\"]\n",
    "num_images_per_keyword = 10\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.google.com/imghp')\n",
    "\n",
    "for keyword in keywords:\n",
    "    search_bar = driver.find_element_by_name('q')\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        driver.implicitly_wait(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    image_tags = soup.find_all('img')\n",
    "\n",
    "    folder_path = f'images/{keyword}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    count = 0\n",
    "    for image in image_tags:\n",
    "        if count == num_images_per_keyword:\n",
    "            break\n",
    "        img_url = image['src']\n",
    "        if img_url.startswith('http'):\n",
    "            response = requests.get(img_url)\n",
    "            file_path = f'{folder_path}/{keyword}_{count}.jpg'\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            count += 1\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b68863",
   "metadata": {},
   "source": [
    "# 4 Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a20fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "search_term = input(\"Enter the smartphone name: \")\n",
    "\n",
    "url = f\"https://www.flipkart.com/search?q={search_term}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "smartphone_list = []\n",
    "for item in soup.select(\"._2kHMtA\"):\n",
    "    try:\n",
    "        brand_name = item.select(\"._4rR01T\")[0].text\n",
    "    except:\n",
    "        brand_name = \"-\"\n",
    "    try:\n",
    "        smartphone_name = item.select(\"._4rR01T\")[1].text\n",
    "    except:\n",
    "        smartphone_name = \"-\"\n",
    "    try:\n",
    "        color = item.select(\".rgWa7D\")[0].text\n",
    "    except:\n",
    "        color = \"-\"\n",
    "    try:\n",
    "        ram = item.select(\".rgWa7D\")[1].text\n",
    "    except:\n",
    "        ram = \"-\"\n",
    "    try:\n",
    "        storage = item.select(\".rgWa7D\")[2].text\n",
    "    except:\n",
    "        storage = \"-\"\n",
    "    try:\n",
    "        primary_camera = item.select(\".rgWa7D\")[3].text\n",
    "    except:\n",
    "        primary_camera = \"-\"\n",
    "    try:\n",
    "        secondary_camera = item.select(\".rgWa7D\")[4].text\n",
    "    except:\n",
    "        secondary_camera = \"-\"\n",
    "    try:\n",
    "        display_size = item.select(\".rgWa7D\")[5].text\n",
    "    except:\n",
    "        display_size = \"-\"\n",
    "    try:\n",
    "        battery_capacity = item.select(\".rgWa7D\")[6].text\n",
    "    except:\n",
    "        battery_capacity = \"-\"\n",
    "    try:\n",
    "        price = item.select(\"._30jeq3._1_WHN1\")[0].text\n",
    "    except:\n",
    "        price = \"-\"\n",
    "    try:\n",
    "        product_url = \"https://www.flipkart.com\" + item.select(\"._1fQZEK\")[0][\"href\"]\n",
    "    except:\n",
    "        product_url = \"-\"\n",
    "    smartphone_list.append([brand_name, smartphone_name, color, ram, storage, primary_camera, secondary_camera, display_size, battery_capacity, price, product_url])\n",
    "\n",
    "df = pd.DataFrame(smartphone_list, columns=[\"Brand Name\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage(ROM)\", \"Primary Camera\", \"Secondary Camera\", \"Display Size\", \"Battery Capacity\", \"Price\", \"Product URL\"])\n",
    "\n",
    "df.to_csv(\"smartphone_details.csv\", index=False)\n",
    "\n",
    "print(\"The details of the smartphones have been saved in a CSV file named 'smartphone_details.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361c2ad",
   "metadata": {},
   "source": [
    "# 5 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43388c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12012172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "image_url = input(\"Enter the URL of the image: \")\n",
    "\n",
    "response = requests.get(image_url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "try:\n",
    "    coordinates = soup.find(\"meta\", {\"itemprop\": \"image\"})[\"content\"].split(\"?center=\")[1].split(\"&zoom=\")[0].split(\",\")\n",
    "    if coordinates:\n",
    "        print(f\"Latitude: {coordinates[0]}, Longitude: {coordinates[1]}\")\n",
    "    else:\n",
    "        print(\"Could not find the coordinates.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e57a85",
   "metadata": {},
   "source": [
    "# 6 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.digit.in/best-laptops/best-gaming-laptops-india-2023\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "laptops = []\n",
    "\n",
    "laptop_elements = soup.find_all(\"div\", class_=\"g-list-item g-card d-card c-product-card\")\n",
    "\n",
    "for laptop_element in laptop_elements:\n",
    "  \n",
    "  name = laptop_element.find(\"h2\", class_=\"c-product-card__title\").text.strip()\n",
    "  price = laptop_element.find(\"div\", class_=\"c-product-card__price\").text.strip()\n",
    "  brand = laptop_element.find(\"a\", class_=\"c-product-card__brand\").text.strip()\n",
    "\n",
    " \n",
    "  specs = {}\n",
    "  spec_table = laptop_element.find(\"table\", class_=\"c-product-card__specs\")\n",
    "  for row in spec_table.find_all(\"tr\"):\n",
    "    key = row.find(\"td\", class_=\"c-product-card__specs-title\").text.strip()\n",
    "    value = row.find(\"td\", class_=\"c-product-card__specs-value\").text.strip()\n",
    "    specs[key] = value\n",
    "\n",
    " \n",
    "  url = laptop_element.find(\"a\", class_=\"c-product-card__image-link\")[\"href\"]\n",
    "\n",
    "\n",
    "  laptops.append({\n",
    "    \"Name\": name,\n",
    "    \"Price\": price,\n",
    "    \"Brand\": brand,\n",
    "    \"Specifications\": specs,\n",
    "    \"URL\": url,\n",
    "  })\n",
    "\n",
    "print(f\"Scraped {len(laptops)} gaming laptops:\")\n",
    "for laptop in laptops:\n",
    "  print(f\"\\nName: {laptop['Name']}\")\n",
    "  print(f\"Price: {laptop['Price']}\")\n",
    "  print(f\"Brand: {laptop['Brand']}\")\n",
    "  print(f\"Specifications:\")\n",
    "  for key, value in laptop['Specifications'].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "  print(f\"URL: {laptop['URL']}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625cf0a2",
   "metadata": {},
   "source": [
    "# 7 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6189e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.forbes.com/forbes-billionaires-list/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "billionaires = []\n",
    "\n",
    "row_elements = soup.find_all(\"tr\", class_=\"browsable-table__row js-click-off\")\n",
    "\n",
    "for row_element in row_elements:\n",
    "  # Rank\n",
    "  rank = row_element.find(\"span\", class_=\"rank\").text.strip()\n",
    "\n",
    "  # Name\n",
    "  name = row_element.find(\"a\", class_=\"btn pfs-profile-link\").text.strip()\n",
    "\n",
    "  # Net worth\n",
    "  net_worth = row_element.find(\"div\", class_=\"net-worth\").text.strip()\n",
    "\n",
    "  # Age\n",
    "  age = row_element.find(\"div\", class_=\"age\").text.strip()\n",
    "\n",
    "  # Citizenship\n",
    "  citizenship = row_element.find(\"div\", class_=\"country\").find(\"img\")[\"title\"] if row_element.find(\"div\", class_=\"country\").find(\"img\") else \"-\"\n",
    "\n",
    "  # Source\n",
    "  source = row_element.find(\"a\", class_=\"source-link\").text.strip() if row_element.find(\"a\", class_=\"source-link\") else \"-\"\n",
    "\n",
    "  # Industry\n",
    "  industry = row_element.find(\"span\", class_=\"tags\").text.strip() if row_element.find(\"span\", class_=\"tags\") else \"-\"\n",
    "\n",
    "  # Append details to list\n",
    "  billionaires.append({\n",
    "    \"Rank\": rank,\n",
    "    \"Name\": name,\n",
    "    \"Net worth\": net_worth,\n",
    "    \"Age\": age,\n",
    "    \"Citizenship\": citizenship,\n",
    "    \"Source\": source,\n",
    "    \"Industry\": industry,\n",
    "  })\n",
    "\n",
    "# Print and optionally save scraped data\n",
    "print(f\"Scraped {len(billionaires)} billionaires:\")\n",
    "for billionaire in billionaires:\n",
    "  print(f\"\\nRank: {billionaire['Rank']}\")\n",
    "  print(f\"Name: {billionaire['Name']}\")\n",
    "  print(f\"Net worth: {billionaire['Net worth']}\")\n",
    "  print(f\"Age: {billionaire['Age']}\")\n",
    "  print(f\"Citizenship: {billionaire['Citizenship']}\")\n",
    "  print(f\"Source: {billionaire['Source']}\")\n",
    "  print(f\"Industry: {billionaire['Industry']}\")\n",
    "\n",
    "# Optionally save data to a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(billionaires)\n",
    "df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622da658",
   "metadata": {},
   "source": [
    "# 8 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgradegoogle-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79eead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"D:/data science/key.json\"\n",
    "\n",
    "\n",
    "# Define API key and video ID\n",
    "api_key = \"\" # Replace with your YouTube API key\n",
    "video_id = \"CfttIk4Yjqg\" # Replace with your desired video ID\n",
    "\n",
    "# Initialize YouTube Data API client\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "# Initialize variables\n",
    "comments = []\n",
    "next_page_token = None\n",
    "\n",
    "# Loop until at least 500 comments are collected\n",
    "while len(comments) < 500:\n",
    "  try:\n",
    "    # Execute API request\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100, # adjust if necessary\n",
    "        pageToken=next_page_token,\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # Extract comment details\n",
    "    for item in response[\"items\"]:\n",
    "      comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "      likes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "      published_at = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "      comments.append({\"comment\": comment, \"likes\": likes, \"published_at\": published_at})\n",
    "\n",
    "    # Check for next page token\n",
    "    next_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "  except HttpError as error:\n",
    "    print(f\"Error retrieving comments: {error}\")\n",
    "    break\n",
    "\n",
    "# Print or process collected comments\n",
    "print(f\"Scraped {len(comments)} comments:\")\n",
    "for comment in comments:\n",
    "  print(f\"\\n- Comment: {comment['comment']}\")\n",
    "  print(f\"  - Upvotes: {comment['likes']}\")\n",
    "  print(f\"  - Posted at: {comment['published_at']}\")\n",
    "\n",
    "# Optionally save comments to a file\n",
    "# with open(\"video_comments.txt\", \"w\") as f:\n",
    "#   for comment in comments:\n",
    "#     f.write(f\"{comment['comment']}\\n\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11470363",
   "metadata": {},
   "source": [
    "# 9 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.hostelworld.com/hostels/england/london\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "hostels = []\n",
    "\n",
    "hostel_elements = soup.find_all(\"div\", class_=\"hw-search-results__item\")\n",
    "\n",
    "for hostel_element in hostel_elements:\n",
    "    \n",
    "  name = hostel_element.find(\"h3\", class_=\"hw-search-results__name\").text.strip()\n",
    "\n",
    "  distance = hostel_element.find(\"span\", class_=\"hw-search-results__location\").text.strip() if hostel_element.find(\"span\", class_=\"hw-search-results__location\") else \"-\"\n",
    "\n",
    "  rating = hostel_element.find(\"span\", class_=\"rating__score\").text.strip() if hostel_element.find(\"span\", class_=\"rating__score\") else \"-\"\n",
    "  reviews = hostel_element.find(\"span\", class_=\"rating__num-reviews\").text.strip() if hostel_element.find(\"span\", class_=\"rating__num-reviews\") else \"-\"\n",
    "\n",
    "  overall_reviews = hostel_element.find(\"span\", class_=\"average-user-rating-badge__count\").text.strip() if hostel_element.find(\"span\", class_=\"average-user-rating-badge__count\") else \"-\"\n",
    "\n",
    "  privates_from = hostel_element.find(\"span\", class_=\"hw-search-results__price\", attrs={\"data-price-type\": \"private\"}).text.strip() if hostel_element.find(\"span\", class_=\"hw-search-results__price\", attrs={\"data-price-type\": \"private\"}) else \"-\"\n",
    "  dorms_from = hostel_element.find(\"span\", class_=\"hw-search-results__price\", attrs={\"data-price-type\": \"dorm\"}).text.strip() if hostel_element.find(\"span\", class_=\"hw-search-results__price\", attrs={\"data-price-type\": \"dorm\"}) else \"-\"\n",
    "\n",
    "  # Facilities\n",
    "  facilities = []\n",
    "  for facility_icon in hostel_element.find_all(\"i\", class_=\"hw-icon\"):\n",
    "    facilities.append(facility_icon[\"aria-label\"].strip())\n",
    "\n",
    "  # Property description\n",
    "  description = hostel_element.find(\"div\", class_=\"hw-search-results__description\").text.strip()\n",
    "\n",
    "  # Append details to list\n",
    "  hostels.append({\n",
    "    \"Name\": name,\n",
    "    \"Distance from City Centre\": distance,\n",
    "    \"Ratings\": rating,\n",
    "    \"Total Reviews\": reviews,\n",
    "    \"Overall Reviews\": overall_reviews,\n",
    "    \"Privates From Price\": privates_from,\n",
    "    \"Dorms From Price\": dorms_from,\n",
    "    \"Facilities\": facilities,\n",
    "    \"Property Description\": description,\n",
    "  })\n",
    "\n",
    "\n",
    "print(f\"Scraped {len(hostels)} hostels in London:\")\n",
    "for hostel in hostels:\n",
    "  print(f\"\\nName: {hostel['Name']}\")\n",
    "  print(f\"Distance from City Centre: {hostel['Distance from City Centre']}\")\n",
    "  print(f\"Ratings: {hostel['Ratings']}\")\n",
    "  print(f\"Total Reviews: {hostel['Total Reviews']}\")\n",
    "  print(f\"Overall Reviews: {hostel['Overall Reviews']}\")\n",
    "  print(f\"Privates From Price: {hostel['Privates From Price']}\")\n",
    "  print(f\"Dorms From Price: {hostel['Dorms From Price']}\")\n",
    "  print(f\"Facilities: {', '.join(hostel['Facilities'])}\")\n",
    "  print(f\"Property Description: {hostel['Property Description']}\")\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc0f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
